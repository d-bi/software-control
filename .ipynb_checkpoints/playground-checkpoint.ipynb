{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook containing **software-control** EEG processing pipeline. Goal is a general purpose discrimination system for EEG data, ie. the ability to classify sample EEG traces to a few given categories.\n",
    "\n",
    "*Authors: Dasheng Bi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data import, preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization, clustering\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Path properties\n",
    "root = 'csv1/'\n",
    "prefix = 'db-rec-'\n",
    "label = ['GO_', 'STOP_']\n",
    "number = range(1, 6)\n",
    "suffix = '.csv'\n",
    "\n",
    "# Data properties\n",
    "SAMPLE_FREQ = 256\n",
    "INTERVAL = 0.5\n",
    "# N_COMPONENTS = 4 # raw data\n",
    "N_COMPONENTS = 20 # filtered DTABG components\n",
    "TRAIN_SAMPLES = 60\n",
    "TEST_SAMPLES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading, Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data import, preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization, clustering\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path properties\n",
    "root = 'csv1/'\n",
    "prefix = 'db-rec-'\n",
    "label = ['GO_', 'STOP_']\n",
    "number = range(1, 6)\n",
    "suffix = '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data, Select Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FREQ = 256\n",
    "INTERVAL = 0.5\n",
    "\n",
    "# N_COMPONENTS = 4 # raw data\n",
    "N_COMPONENTS = 20 # filtered DTABG components\n",
    "\n",
    "TRAIN_SAMPLES = 60\n",
    "TEST_SAMPLES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root + prefix + label[0] + str(number[0]) + suffix\n",
    "df = pd.read_csv(path)\n",
    "print(df.columns)\n",
    "df = df.drop(columns=df.columns[21:])\n",
    "print(df.columns)\n",
    "df = df.drop(columns=df.columns[0])\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_stack = np.empty((0, int(INTERVAL * SAMPLE_FREQ * N_COMPONENTS)))\n",
    "# print(data_stack)\n",
    "# print(data_stack.shape)\n",
    "for l in label:\n",
    "    for trial in number:\n",
    "        path = root + prefix + l + str(trial) + suffix\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "#         df = df.drop(columns=df.columns[25:])\n",
    "#         df = df.drop(columns=df.columns[0:21])  # Only keep raw EEG data.\n",
    "\n",
    "        df = df.drop(columns=df.columns[21:]) # drop raw columns\n",
    "        df = df.drop(columns=df.columns[0]) # Keep only DTABG filtered components.\n",
    "\n",
    "        if (df.equals(df.dropna()) == False):\n",
    "            print(\"Dropped NaN: \\n\", np.argwhere(np.isnan(df.values)))\n",
    "            df = df.dropna()  # Drop NaN (blank) datapts.\n",
    "\n",
    "        OFFSET = 1\n",
    "        while OFFSET < 7:\n",
    "            START = int(OFFSET * SAMPLE_FREQ)\n",
    "            END = int(START + SAMPLE_FREQ * INTERVAL)\n",
    "            OFFSET += 0.5\n",
    "\n",
    "            raw = df.iloc[START:END]  # Select by row\n",
    "            raw_vals = raw.values\n",
    "    #         print(np.argwhere(np.isnan(raw_vals)))\n",
    "            raw_vals = np.reshape(\n",
    "                raw_vals, (1, int(INTERVAL * SAMPLE_FREQ * N_COMPONENTS)))\n",
    "            data_stack = np.concatenate((data_stack, raw_vals), axis=0)\n",
    "#         print(data_stack.shape)\n",
    "df_stack = pd.DataFrame(data_stack)\n",
    "# print(df_stack.head())\n",
    "# num_rows=num_trials; num_cols=num_datapts\n",
    "print(\"data_stack shape: \", data_stack.shape)\n",
    "print(\"confirm no NaN:\", len(np.argwhere(np.isnan(data_stack))) == 0)  # Should be empty\n",
    "\n",
    "# Labels\n",
    "train_labels = [1] * TRAIN_SAMPLES + [0] * TRAIN_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stack = np.empty((0, int(INTERVAL * SAMPLE_FREQ * N_COMPONENTS)))\n",
    "for l in label:\n",
    "    for trial in range(2, 6):\n",
    "        path = root + prefix + l + str(trial) + suffix\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "#         df = df.drop(columns=df.columns[25:])\n",
    "#         df = df.drop(columns=df.columns[0:21])  # Only keep raw EEG data.\n",
    "\n",
    "        df = df.drop(columns=df.columns[21:]) # drop raw columns\n",
    "        df = df.drop(columns=df.columns[0]) # Keep only DTABG filtered components.\n",
    "        \n",
    "        if (df.equals(df.dropna()) == False):\n",
    "            print(\"Dropped NaN: \\n\", np.argwhere(np.isnan(df.values)))\n",
    "            df = df.dropna()  # Drop NaN (blank) datapts.\n",
    "\n",
    "        OFFSET = 7\n",
    "        while OFFSET < 8:\n",
    "            START = int(OFFSET * SAMPLE_FREQ)\n",
    "            END = int(START + SAMPLE_FREQ * INTERVAL)\n",
    "            OFFSET += 0.5\n",
    "\n",
    "            raw = df.iloc[START:END]  # Select by row\n",
    "            raw_vals = raw.values\n",
    "            raw_vals = np.reshape(\n",
    "                raw_vals, (1, int(INTERVAL * SAMPLE_FREQ * N_COMPONENTS)))\n",
    "            test_stack = np.concatenate((test_stack, raw_vals), axis=0)\n",
    "#             print(test_stack.shape)\n",
    "df_test = pd.DataFrame(test_stack)\n",
    "# print(df_stack.head())\n",
    "# num_rows=num_trials; num_cols=num_datapts\n",
    "print(\"test_stack shape: \", test_stack.shape)\n",
    "print(\"confirm no NaN:\", len(np.argwhere(np.isnan(test_stack))) == 0)  # Should be empty\n",
    "\n",
    "# Labels\n",
    "test_labels = [1] * TEST_SAMPLES + [0] * TEST_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Clustering of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1250)\n",
    "tsne_results = tsne.fit_transform(data_stack)\n",
    "\n",
    "# Plot\n",
    "df_stack['tsne-2d-one'] = tsne_results[:, 0]\n",
    "df_stack['tsne-2d-two'] = tsne_results[:, 1]\n",
    "# Labels\n",
    "NUM_SAMPLES_PER_LABEL = 60\n",
    "y = [1] * NUM_SAMPLES_PER_LABEL + [0] * NUM_SAMPLES_PER_LABEL\n",
    "df_stack['y'] = y\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 2),\n",
    "    data=df_stack,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground: Multiple t-SNEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10), constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=5, nrows=5, figure=fig)\n",
    "\n",
    "for niter in range(250, 1500, 250):\n",
    "    for perp in range(10, 55, 10):\n",
    "        # multiple t-SNEs\n",
    "        tsne = TSNE(n_components=2, verbose=1, perplexity=perp, n_iter=niter)\n",
    "        tsne_results = tsne.fit_transform(data_stack)\n",
    "\n",
    "        coord_prefix = 'tsne-2d-'\n",
    "        coord_desc = str(niter) + '-' + str(perp)\n",
    "        x_coord = coord_prefix + coord_desc + '-one'\n",
    "        y_coord = coord_prefix + coord_desc + '-two'\n",
    "        df_stack[x_coord] = tsne_results[:, 0]\n",
    "        df_stack[y_coord] = tsne_results[:, 1]\n",
    "\n",
    "        # Labels\n",
    "        NUM_SAMPLES_PER_LABEL = 60\n",
    "        y = [1] * NUM_SAMPLES_PER_LABEL + [0] * NUM_SAMPLES_PER_LABEL\n",
    "        df_stack['y'] = y\n",
    "\n",
    "        niter_offset = int((niter-250)/250)\n",
    "        perp_offset = int((perp-10)/10)\n",
    "        fig.add_subplot(spec[niter_offset, perp_offset])\n",
    "        sns.scatterplot(\n",
    "            x=x_coord, y=y_coord,\n",
    "            hue=\"y\",\n",
    "            palette=sns.color_palette(\"hls\", 2),\n",
    "            data=df_stack,\n",
    "            legend=\"full\",\n",
    "            alpha=0.8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Clustering of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca_result = pca.fit_transform(data_stack)\n",
    "df_stack['pca-one'] = pca_result[:,0]\n",
    "df_stack['pca-two'] = pca_result[:,1] \n",
    "df_stack['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "print('Sum:', sum(pca.explained_variance_ratio_))\n",
    "## Explained variation per principal component: [0.1557712  0.11314638 0.08842925]\n",
    "\n",
    "# Labels\n",
    "NUM_SAMPLES_PER_LABEL = 60\n",
    "y = [1] * NUM_SAMPLES_PER_LABEL + [0] * NUM_SAMPLES_PER_LABEL\n",
    "df_stack['y'] = y\n",
    "\n",
    "# Plot\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=df_stack[\"pca-one\"], \n",
    "    ys=df_stack[\"pca-two\"], \n",
    "    zs=df_stack[\"pca-three\"], \n",
    "    c=df_stack[\"y\"], \n",
    "    cmap='jet'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference via KNN in PCA Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Sample into PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_pca(raw_eeg, pca_components):\n",
    "    \"\"\"\n",
    "    Embeds RAW_EEG into the PCA representation space given by PCA_COMPONENTS\n",
    "    \"\"\"\n",
    "    return np.dot(pca_components, raw_eeg.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_components = pca.components_\n",
    "# print(pca_components.shape)\n",
    "# data_stack_pca = np.dot(pca_components, data_stack.T).T\n",
    "data_stack_pca = embed_pca(data_stack, pca.components_)\n",
    "print(data_stack_pca.shape)\n",
    "test_stack_pca = embed_pca(test_stack, pca.components_)\n",
    "print(test_stack_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground: Embedding one sample into PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure for obtaining one sample's representation in PCA space\n",
    "sample = data_stack[0,:]\n",
    "print(sample.shape)\n",
    "sample_rep = np.dot(pca_components, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground: Test Different k-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 51)\n",
    "scores_list = []\n",
    "TEST_TO_TRAIN_WEIGHT = 20\n",
    "total_stack_pca = np.concatenate(\n",
    "    (np.tile(test_stack_pca,\n",
    "             (TEST_TO_TRAIN_WEIGHT, 1)),\n",
    "     data_stack_pca))\n",
    "total_labels = np.concatenate(\n",
    "    (np.tile(test_labels,\n",
    "             (TEST_TO_TRAIN_WEIGHT)),\n",
    "     train_labels))\n",
    "print(total_stack_pca.shape)\n",
    "print(total_labels.shape)\n",
    "for k_val in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_val)\n",
    "    knn.fit(data_stack_pca, train_labels)\n",
    "    knn_predict = knn.predict(test_stack_pca)\n",
    "    scores_list.append(metrics.accuracy_score(test_labels, knn_predict))\n",
    "\n",
    "print('Maximum accuracy is %f' % (np.amax(scores_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot KNN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=k_range, y=scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data to tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read into Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped NaN: \n",
      " [[187   0]\n",
      " [187   1]\n",
      " [187   2]\n",
      " [187   3]\n",
      " [187   4]\n",
      " [187   5]\n",
      " [187   6]\n",
      " [187   7]\n",
      " [187   8]\n",
      " [187   9]\n",
      " [187  10]\n",
      " [187  11]\n",
      " [187  12]\n",
      " [187  13]\n",
      " [187  14]\n",
      " [187  15]\n",
      " [187  16]\n",
      " [187  17]\n",
      " [187  18]\n",
      " [187  19]]\n",
      "data_stack shape:  (120, 128, 20)\n",
      "confirm no NaN: True\n"
     ]
    }
   ],
   "source": [
    "data_stack = np.empty((0, int(INTERVAL * SAMPLE_FREQ), N_COMPONENTS))\n",
    "# print(data_stack.shape)\n",
    "\n",
    "for l in label:\n",
    "    for trial in number:\n",
    "        path = root + prefix + l + str(trial) + suffix\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "#         df = df.drop(columns=df.columns[25:])\n",
    "#         df = df.drop(columns=df.columns[0:21])  # Only keep raw EEG data.\n",
    "\n",
    "        df = df.drop(columns=df.columns[21:]) # drop raw columns\n",
    "        df = df.drop(columns=df.columns[0]) # Keep only DTABG filtered components.\n",
    "        \n",
    "#         print(df.shape)\n",
    "\n",
    "        if (df.equals(df.dropna()) == False):\n",
    "            print(\"Dropped NaN: \\n\", np.argwhere(np.isnan(df.values)))\n",
    "            df = df.dropna()  # Drop NaN (blank) datapts.\n",
    "\n",
    "        OFFSET = 1\n",
    "        while OFFSET < 7:\n",
    "            START = int(OFFSET * SAMPLE_FREQ)\n",
    "            END = int(START + SAMPLE_FREQ * INTERVAL)\n",
    "            OFFSET += 0.5\n",
    "\n",
    "            raw = df.iloc[START:END]  # Select by row\n",
    "            raw_vals = raw.values\n",
    "            raw_vals = np.reshape(\n",
    "                raw_vals, (1, int(INTERVAL * SAMPLE_FREQ), N_COMPONENTS))\n",
    "            data_stack = np.concatenate((data_stack, raw_vals), axis=0)\n",
    "#         print(data_stack.shape)\n",
    "        \n",
    "print(\"data_stack shape: \", data_stack.shape)\n",
    "print(\"confirm no NaN:\", len(np.argwhere(np.isnan(data_stack))) == 0)  # Should be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped NaN: \n",
      " [[187   0]\n",
      " [187   1]\n",
      " [187   2]\n",
      " [187   3]\n",
      " [187   4]\n",
      " [187   5]\n",
      " [187   6]\n",
      " [187   7]\n",
      " [187   8]\n",
      " [187   9]\n",
      " [187  10]\n",
      " [187  11]\n",
      " [187  12]\n",
      " [187  13]\n",
      " [187  14]\n",
      " [187  15]\n",
      " [187  16]\n",
      " [187  17]\n",
      " [187  18]\n",
      " [187  19]]\n",
      "test_stack shape:  (16, 128, 20)\n",
      "confirm no NaN: True\n"
     ]
    }
   ],
   "source": [
    "test_stack = np.empty((0, int(INTERVAL * SAMPLE_FREQ), N_COMPONENTS))\n",
    "for l in label:\n",
    "    for trial in range(2, 6):\n",
    "        path = root + prefix + l + str(trial) + suffix\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "#         df = df.drop(columns=df.columns[25:])\n",
    "#         df = df.drop(columns=df.columns[0:21])  # Only keep raw EEG data.\n",
    "\n",
    "        df = df.drop(columns=df.columns[21:]) # drop raw columns\n",
    "        df = df.drop(columns=df.columns[0]) # Keep only DTABG filtered components.\n",
    "        \n",
    "        if (df.equals(df.dropna()) == False):\n",
    "            print(\"Dropped NaN: \\n\", np.argwhere(np.isnan(df.values)))\n",
    "            df = df.dropna()  # Drop NaN (blank) datapts.\n",
    "\n",
    "        OFFSET = 7\n",
    "        while OFFSET < 8:\n",
    "            START = int(OFFSET * SAMPLE_FREQ)\n",
    "            END = int(START + SAMPLE_FREQ * INTERVAL)\n",
    "            OFFSET += 0.5\n",
    "\n",
    "            raw = df.iloc[START:END]  # Select by row\n",
    "            raw_vals = raw.values\n",
    "            raw_vals = np.reshape(\n",
    "                raw_vals, (1, int(INTERVAL * SAMPLE_FREQ), N_COMPONENTS))\n",
    "            test_stack = np.concatenate((test_stack, raw_vals), axis=0)\n",
    "#             print(test_stack.shape)\n",
    "# df_test = pd.DataFrame(test_stack)\n",
    "# print(df_stack.head())\n",
    "# num_rows=num_trials; num_cols=num_datapts\n",
    "print(\"test_stack shape: \", test_stack.shape)\n",
    "print(\"confirm no NaN:\", len(np.argwhere(np.isnan(test_stack))) == 0)  # Should be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "train_labels = np.concatenate(\n",
    "    (np.tile([0, 1], (TRAIN_SAMPLES, 1)),\n",
    "     np.tile([1, 0], (TRAIN_SAMPLES, 1)))\n",
    ")\n",
    "test_labels = np.concatenate(\n",
    "    (np.tile([0, 1], (TEST_SAMPLES, 1)),\n",
    "     np.tile([1, 0], (TEST_SAMPLES, 1)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf.data.Dataset from tensor slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET_SIZE = 120\n",
    "TRAIN_SIZE = int(FULL_DATASET_SIZE * 0.8)\n",
    "VALIDATION_SIZE = int(FULL_DATASET_SIZE * 0.2)\n",
    "\n",
    "# TRAIN_BATCH_SIZE = 4\n",
    "# VALIDATION_BATCH_SIZE = 1\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_stack, train_labels)\n",
    ").shuffle(FULL_DATASET_SIZE)\n",
    "\n",
    "train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "validation_dataset = full_dataset.skip(TRAIN_SIZE).take(VALIDATION_SIZE)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.random.shuffle(data_stack)[:VALIDATION_SIZE,:,:], train_labels[:VALIDATION_SIZE,:,:])\n",
    ").batch(VALIDATION_BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_stack, test_labels))\n",
    "TEST_SHUFFLE_BUFFER = 16\n",
    "TEST_BATCH_SIZE = 1\n",
    "test_dataset = test_dataset.shuffle(TEST_SHUFFLE_BUFFER).batch(TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 128, 20), (None, 2)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_conv\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 20, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 126, 18, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 63, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 61, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 30, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,630,066\n",
      "Trainable params: 1,630,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (int(SAMPLE_FREQ * INTERVAL), N_COMPONENTS, 1)\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "# A proposed architecture is to use recurrent first to \"su16marize,\"\n",
    "    # No need for embedding layer in recurrent subnetwork?\n",
    "# then use convolutional to classify.\n",
    "\n",
    "\n",
    "first_conv = layers.Conv2D(16, 3, activation='relu')\n",
    "first_pool = layers.MaxPool2D(pool_size=(2, 1))\n",
    "second_conv = layers.Conv2D(32, 3, activation='relu')\n",
    "second_pool = layers.MaxPool2D(pool_size=(2, 1))\n",
    "third_conv = layers.Conv2D(64, 3, activation='relu')\n",
    "flattener = layers.Flatten()\n",
    "first_dense = layers.Dense(64, activation='relu')\n",
    "second_dense = layers.Dense(16, activation='relu')\n",
    "third_dense = layers.Dense(2)\n",
    "\n",
    "outputs = third_dense(\n",
    "    second_dense(first_dense(\n",
    "        flattener(\n",
    "            third_conv(\n",
    "                second_pool(second_conv(\n",
    "                    first_pool(first_conv(inputs)))))))))\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='simple_conv')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.5521"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2eb315af5a40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\think\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\think\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[0;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[1;32m--> 862\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\think\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\think\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1089\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10, verbose=1, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "613.2px",
    "left": "43px",
    "top": "110.8px",
    "width": "221.597px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 378.17572800000005,
   "position": {
    "height": "398.898px",
    "left": "798.364px",
    "right": "20px",
    "top": "20px",
    "width": "583.352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
